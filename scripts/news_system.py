import os
import json
import random
import requests
import re
from datetime import datetime
from dotenv import load_dotenv

class LLMClient:
    def __init__(self):
        # åˆå§‹åŒ–APIç«¯ç‚¹
        self.url = "https://models.sjtu.edu.cn/api/v1/chat/completions"
        # é»˜è®¤ä½¿ç”¨å¿«æ¨¡å‹ (ç”¨äºæ¯å°æ—¶å¿«è®¯/ç‚¹è¯„)
        self.default_model = "qwen3vl" 
        
    def get_api_key(self):
        load_dotenv()
        api_key = os.getenv("DEEPSEEK_API_KEY") 
        return api_key

    def chat(self, prompt, system_prompt="", model=None):
        """
        å‘é€è¯·æ±‚ç»™ LLM
        :param model: å¦‚æœæŒ‡å®šï¼Œåˆ™è¦†ç›–é»˜è®¤æ¨¡å‹ (ä¾‹å¦‚å¼ºåˆ¶ç”¨ deepseek-r1)
        """
        api_key = self.get_api_key()
        if not api_key: return None

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        target_model = model if model else self.default_model

        if not system_prompt:
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èå¸‚åœºåˆ†æå¸ˆï¼Œé£æ ¼çŠ€åˆ©ã€ç®€ç»ƒã€‚"

        payload = {
            "model": target_model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "temperature": 1.0, 
            # å¦‚æœæ˜¯ DeepSeek-R1ï¼Œç»™è¶³ Token è®©ä»–æ€è€ƒ
            "max_tokens": 2000 if "deepseek" in target_model else 1000 
        }

        try:
            # R1 æ¯”è¾ƒæ…¢ï¼Œå¦‚æœæ˜¯ R1 åˆ™ç»™ 60ç§’è¶…æ—¶ï¼Œæ™®é€šæ¨¡å‹ 30ç§’
            timeout = 60 if "deepseek" in target_model else 30
            
            # print(f"[DEBUG] Calling {target_model} (Timeout: {timeout}s)...")
            
            response = requests.post(self.url, headers=headers, json=payload, timeout=timeout)
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                # æ¸…æ´— DeepSeek çš„æ€è€ƒè¿‡ç¨‹ <think>...</think>
                content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
                return content
            else:
                print(f"API Error: {response.status_code} - {response.text}")
                return None
        except Exception as e:
            print(f"LLM Failed ({target_model}): {e}")
            return None

# å…¨å±€å®ä¾‹
llm_client = LLMClient()

def generate_news(news_type):
    """(ä¿æŒä½¿ç”¨é»˜è®¤å¿«æ¨¡å‹)"""
    prompt = f"ç”Ÿæˆä¸€æ¡å…³äºè‚¡å¸‚çš„ã€é‡å¤§{news_type}ã€‘å¿«è®¯ã€‚è¦æ±‚ï¼š30å­—ä»¥å†…ï¼Œæ¨¡ä»¿å½­åšç¤¾é£æ ¼ï¼ŒåŒ…å«å…·ä½“è™šæ„äº‹ä»¶ï¼ˆå¦‚èŠ¯ç‰‡ã€æˆ˜äº‰ã€è´¢æŠ¥ï¼‰ã€‚ç›´æ¥è¾“å‡ºæ ‡é¢˜ã€‚"
    res = llm_client.chat(prompt)
    if res: return res
    return "é‡ç£…ï¼šå¸‚åœºå‡ºç°å‰§çƒˆæ³¢åŠ¨ï¼Œç¥ç§˜èµ„é‡‘æ­£åœ¨é€šè¿‡æš—æ± è¿›è¡Œå¤§è§„æ¨¡äº¤æ˜“"

def generate_hourly_comment(hour, price, change_pct, volume):
    """(ä¿æŒä½¿ç”¨é»˜è®¤å¿«æ¨¡å‹)"""
    prompt = f"""
    å½“å‰æ˜¯ç¬¬ {hour} å°æ—¶äº¤æ˜“ç»“æŸã€‚
    è‚¡ä»·: ${price:.2f}
    æœ¬å°æ—¶æ¶¨è·Œå¹…: {change_pct:+.2f}%
    æˆäº¤é‡: {volume} æ‰‹
    
    è¯·ç”¨ã€ä¸€å¥è¯ã€‘ç‚¹è¯„å½“å‰ç›˜é¢æƒ…ç»ªï¼ˆææ…Œ/è´ªå©ª/è§‚æœ›ï¼‰ã€‚30å­—ä»¥å†…ï¼ŒçŠ€åˆ©ä¸€ç‚¹ã€‚
    """
    res = llm_client.chat(prompt)
    return res if res else f"å¸‚åœºæ³¢åŠ¨å‰§çƒˆï¼Œå¤šç©ºåŒæ–¹åœ¨ ${price:.2f} å±•å¼€æ¿€çƒˆäº‰å¤ºã€‚"

def generate_end_game_summary(stats):
    """
    ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç»“å±€åˆ†æ - å¼•å…¥æ“ç›˜æ‰‹æ”¶å‰²KPIç‚¹è¯„
    """
    start_price = stats["start_price"]
    end_price = stats["end_price"]
    change = ((end_price - start_price) / start_price) * 100
    
    winner = stats["top_player"]
    mm_names = ", ".join(stats["mm_names"])
    
    # æ„å»º Prompt ä¸Šä¸‹æ–‡
    context = f"""
    ã€æš—ä»“æ¸¸æˆç»“ç®—æ•°æ®ã€‘
    - è‚¡ä»·èµ°åŠ¿: ${start_price:.2f} -> ${end_price:.2f} (æ¶¨è·Œå¹… {change:+.2f}%)
    - èµ„äº§å† å†›: {winner.display_name} (èº«ä»½: {winner.role}, èµ„äº§: ${winner.cash:,.2f})
    - æ•£æˆ·é˜µäº¡äººæ•°: {stats['losers_count']} äºº
    - æ“ç›˜æ‰‹åå•: {mm_names}
    
    ã€æ“ç›˜æ‰‹(åº„å®¶) ç»©æ•ˆè€ƒæ ¸ã€‘
    - ç›®æ ‡æ”¶å‰²é‡‘é¢: ${stats['harvest_target']:,.0f}
    - å®é™…é€ æˆæ•£æˆ·äºæŸ: ${stats['total_retail_loss']:,.0f}
    - è€ƒæ ¸ç»“æœ: {"âœ… æ”¶å‰²æˆåŠŸ (è¡€æµæˆæ²³)" if stats['mm_success'] else "âŒ æ”¶å‰²å¤±è´¥ (æ•£æˆ·ç”šè‡³èµšäº†)"}
    """

    prompt = f"""
    {context}
    
    è¯·å†™ä¸€æ®µ 200å­—å·¦å³ çš„ã€å¸‚åœºæ”¶ç›˜æ€»ç»“ã€‘ï¼Œé£æ ¼æ¨¡ä»¿ã€Šåå°”è¡—ä¹‹ç‹¼ã€‹ï¼Œæå°½å˜²è®½ä¸å†·é…·ã€‚
    
    ã€å¼ºåˆ¶è¦æ±‚ã€‘
    1. **é‡ç‚¹ç‚¹è¯„æ“ç›˜æ‰‹ ({mm_names}) çš„è¡¨ç°**ï¼š
       - å¦‚æœè€ƒæ ¸ç»“æœæ˜¯ã€æˆåŠŸã€‘ï¼šç§°èµä»–ä»¬æ˜¯å†·è¡€çš„å± å¤«ï¼ŒæˆåŠŸæŠŠæ•£æˆ·å˜æˆäº†ç‡ƒæ–™ï¼Œå¸‚åœºå°±æ˜¯é›¶å’Œåšå¼ˆçš„å± å®°åœºã€‚
       - å¦‚æœè€ƒæ ¸ç»“æœæ˜¯ã€å¤±è´¥ã€‘ï¼š**æ— æƒ…å˜²è®½æ“ç›˜æ‰‹**ï¼è¯´ä»–ä»¬æ˜¯"åƒç´ çš„ç‹¼"ã€"è¢«æ•£æˆ·åæ€çš„åºŸç‰©"ï¼Œå³ä½¿ä»–ä»¬èµ„äº§å¾ˆé«˜ï¼Œä½†æ²¡èƒ½è®©æ•£æˆ·äºé’±å°±æ˜¯åº„å®¶çš„è€»è¾±ï¼
    
    2. å¿…é¡»å¼•ç”¨æ•°æ®ï¼šæ˜ç¡®æåˆ°æ•£æˆ·æ€»å…±äºæŸäº† ${stats['total_retail_loss']:,.0f}ã€‚
    3. ç»“å°¾è¦å‡åï¼šå…³äºè´ªå©ªã€ææƒ§å’Œä¿¡æ¯å·®çš„æ®‹é…·çœŸç†ã€‚
    4. ä¸è¦å‡ºç°â€œï¼ˆå­—æ•°ï¼šXXXï¼‰â€ã€‚
    """
    
    print(f"[ç³»ç»Ÿ] æ­£åœ¨è°ƒç”¨ DeepSeek-R1 ç”Ÿæˆæ·±åº¦æˆ˜æŠ¥ (å«æ“ç›˜æ‰‹ç‚¹è¯„)...")
    res = llm_client.chat(prompt, model="deepseek-r1")
    
    # å…œåº•ï¼šé˜²æ­¢ AI æ¼æ‰å…³é”®æ•°æ®
    if res and str(int(stats['total_retail_loss'])) not in res.replace(",",""):
         res += f" (æ³¨ï¼šæœ¬æ¬¡æ“ç›˜æ‰‹å…±ä»æ•£æˆ·èº«ä¸Šæ¦¨å–äº† ${stats['total_retail_loss']:,.2f} çš„è¡€æ±—é’±ã€‚)"
        
    return res if res else "äº¤æ˜“ç»“æŸã€‚æ®‹é…·çš„å¸‚åœºå†æ¬¡è¯æ˜ï¼Œèµ„æœ¬çš„åŸå§‹ç§¯ç´¯æ€»æ˜¯ä¼´éšç€è¡€è…¥ã€‚"

def format_news_for_display(content, tag="ğŸ“¢"):
    timestamp = datetime.now().strftime("%H:%M")
    return f"[{timestamp}] {tag} {content}"